# CSE 476 - Final Project

This repository contains my final project submission for CSE476: an inference-time agent that solves reasoning tasks using the provided ASU LLM API and the supplied development data.

Summary
-------

This project implements an inference-time agent that selects reasoning strategies, calls the provided ASU LLM API, and extracts a concise final answer for each input question. The repo includes a small agent scaffold, a template script to generate answers over a dataset, and basic instructions to reproduce results.

Key files
---------

- `agent.py` — agent loop, strategies, and API client scaffold.
- `generate_answer_template.py` — template script; edit `build_answers()` to plug in your agent logic.
- `cse476_final_project_dev_data.json` — development dataset (provided).
- `cse_476_final_project_test_data.json` — final test dataset (released by instructors).
- `cse_476_final_project_answers.json` — expected output file generated by the agent.
- `requirements.txt` — Python dependencies.

How the agent works (high level)
--------------------------------

1) Request handling — a single LLM-call wrapper handles requests and errors.
2) Strategy selection — classify the question and pick a strategy: direct answer, chain-of-thought (CoT), self-refinement, or domain-specialized variants (coding/planning).
3) Final-answer extraction — parse the model output for an explicit marker like `Final Answer:` and return the concise answer string; otherwise return the stripped response.

Core strategies
-------------------------------------

- **Direct Answering**: one deterministic call (temperature=0) that returns the model's answer.
- **Chain-of-Thought Reasoning (CoT)**: elicits step-by-step reasoning and applies a short extractor pass when the reasoning is long to obtain a one-line final answer.
- **Self-Refinement**: produce an initial answer and ask the model to critique and correct it.

Default policy
--------------

For many problems the agent uses CoT followed by self-refinement to improve answer correctness. For short, unambiguous items it uses the direct strategy for efficiency.

Reproducibility & constraints
-----------------------------

- Uses only the provided ASU API for LLM calls — no paid external services.  
- Designed to be efficient: target <20 LLM calls per question for typical inputs.  
- The scaffold falls back to a local placeholder when API variables are not set so you can test offline.

Run instructions (quick)
----------------------

1. Create + activate a virtual environment (recommended):

```powershell
python -m venv .venv
& .\.venv\Scripts\Activate.ps1
```

2. Install dependencies:

```powershell
python -m pip install -r requirements.txt
```

3. (Optional) Set API environment variables to use the provided ASU API:

```powershell
$env:OPENAI_API_KEY = "<key-if-required>"
$env:API_BASE = "http://10.4.58.53:41701/v1"
$env:MODEL_NAME = "bens_model"
```

4. Import agent and run:

You can import the agent's run function and call it directly from Python. Example:

```python
from agent import run_agent

prompt = "Your question here"
ans = run_agent(prompt)
print(ans)
```

When generating answers over a dataset (as in `generate_answer_template.py`) the pattern used is:

```python
real_answer = run_agent(question["input"])
answers.append({"output": real_answer})
```

Implementation notes
--------------------------------

- Development used the provided dev dataset to iterate on prompts and strategies; outputs were compared to `expected_output` using simple normalized-string matching.  
- Improvements focused on prompt design for CoT, a short extraction pass to ensure concise final answers, and a lightweight self-refinement loop.