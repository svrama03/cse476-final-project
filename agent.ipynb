{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1e24c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os, json, textwrap, re, time\n",
    "import requests\n",
    "from typing import Tuple\n",
    "\n",
    "# Environment/configuration (override via env vars)\n",
    "API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")\n",
    "MODEL    = os.getenv(\"MODEL_NAME\", \"bens_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a440df7",
   "metadata": {},
   "source": [
    "## API helper: `call_model_chat_completions`\n",
    "\n",
    "This function calls an OpenAI-style `/v1/chat/completions` endpoint and returns a structured result. It is safe to run when the ASU API is reachable (on-campus or via VPN) and the `API_BASE`/`API_KEY` environment variables are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7afcc043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"You are a helpful assistant. Reply with only the final answerâ€”no explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                temperature: float = 0.0,\n",
    "                                max_tokens: int = 128,\n",
    "                                timeout: int = 60) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs   = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555ec4c",
   "metadata": {},
   "source": [
    "## Strategy 1 - Direct Answer (Single LLM Call)\n",
    "- Gives the model a strict system prompt ensuring it only outputs the final answer\n",
    "- Makes one call to the API\n",
    "- Strips whitespace and returns result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "449f1b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct(question: str) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Strategy 1: Directly ask the model the question.\n",
    "    \"\"\"\n",
    "    directPrompt = \"You are a helpful assistant, reply with only the final answer and give no explanations.\"\n",
    "    systemResponse = call_model_chat_completions(question, system=directPrompt, temperature=0)\n",
    "    \n",
    "    if systemResponse.get('ok'):\n",
    "        return True, systemResponse.get('text', '').strip()\n",
    "    else:\n",
    "        return False, systemResponse.get('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac858c89",
   "metadata": {},
   "source": [
    "## Strategy 2 - Chain of Thought Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_of_thought(question: str) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Chain-of-Thought prompting.\n",
    "    \"\"\"\n",
    "    cotPrompt = textwrap.dedent(f\"\"\"\\\n",
    "    You are a helpful assistant. Think through the problem step-by-step before providing the final answer.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Let's think step by step.\"\"\")\n",
    "    \n",
    "    systemResponse = call_model_chat_completions(cotPrompt, temperature=0)\n",
    "    \n",
    "    if systemResponse.get('ok'):\n",
    "        return True, systemResponse.get('text', '').strip()\n",
    "    else:\n",
    "        return False, systemResponse.get('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f97eb",
   "metadata": {},
   "source": [
    "## Strategy 3 - Self Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4a0681e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_refine(question: str, initial_answer: str) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Strategy 3: Self-Refinement - The model reviews and refines its own answer.\n",
    "    \"\"\"\n",
    "    systemPrompt = textwrap.dedent(\n",
    "        \"You are a meticulous assistant skilled in reviewing and refining answers. \"\\\n",
    "        \"Your task is to evaluate the provided answer to the question and improve it if necessary. \"\\\n",
    "        \"You will compare the initial answer against the question, identify any errors or omissions, \"\\\n",
    "        \"and provide a corrected and enhanced answer. \"\\\n",
    "        \"Follow these steps:\\n\"\n",
    "        \"1. Analyze the question and the initial answer carefully.\\n\"\n",
    "        \"2. Determine if the initial answer is correct and complete.\\n\"\n",
    "        \"3. If the answer is correct, repeat the correct answer. \"\\\n",
    "        \"Otherwise, provide only a corrected answer without any explanation.\"\n",
    "    )\n",
    "    selfRefinePrompt = textwrap.dedent(f\"\"\"\\\n",
    "        Question: {question}\n",
    "        Initial Answer: {initial_answer}\n",
    "        TASKS:\n",
    "        Please provide a refined answer:\n",
    "    \"\"\"\n",
    "    )\n",
    "    systemResponse = call_model_chat_completions(selfRefinePrompt, system=systemPrompt, temperature=0)\n",
    "    if systemResponse.get('ok'):\n",
    "        return True, systemResponse.get('text', '').strip()\n",
    "    else:\n",
    "        return False, systemResponse.get('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0985fe2",
   "metadata": {},
   "source": [
    "## Agent Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dc944666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_question(question: str) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Classify the question to determine the best strategy.\n",
    "    \"\"\"\n",
    "    systemPrompt = \"You are an expert question classifier. Analyze the question and determine the most suitable answering strategy: Direct, Chain-of-Thought, or Self-Refinement. Respond with only the strategy name.\"\n",
    "    classifyPrompt = textwrap.dedent(f\"\"\"\\\n",
    "        Question: {question}\n",
    "        TASK:\n",
    "        Determine the best strategy to answer the question: Direct, Chain-of-Thought, or Self-Refinement.\n",
    "        Respond with only the strategy name.\n",
    "    \"\"\")\n",
    "    classifyResponse = call_model_chat_completions(classifyPrompt, system=systemPrompt, temperature=0)\n",
    "    if classifyResponse.get('ok'):\n",
    "        return True, classifyResponse.get('text', '').strip()\n",
    "    else:\n",
    "        return False, classifyResponse.get('error')\n",
    "    \n",
    "def run_agent(question: str) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Run the agent to answer the question using the classified strategy.\n",
    "    \"\"\"\n",
    "    success, strategy = classify_question(question)\n",
    "    if not success:\n",
    "        return False, f\"Classification Error: {strategy}\"\n",
    "    \n",
    "    strategy = strategy.lower()\n",
    "    if \"direct\" in strategy:\n",
    "        return direct(question)\n",
    "    elif \"chain-of-thought\" in strategy or \"chain of thought\" in strategy:\n",
    "        return chain_of_thought(question)\n",
    "    elif \"self-refine\" in strategy or \"self refine\" in strategy:\n",
    "        # First get an initial answer\n",
    "        success, initial_answer = direct(question)\n",
    "        if not success:\n",
    "            return False, f\"Initial Answer Error: {initial_answer}\"\n",
    "        # Then refine it\n",
    "        return self_refine(question, initial_answer)\n",
    "    else:\n",
    "        return False, f\"Unknown Strategy: {strategy}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "51396449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Gunmen from Laredo starred which narrator of \"Frontier\"?\n",
      "Answer: <thinking>\n",
      "<steps>\n",
      "<step>\n",
      "<expert>Physicist</expert>\n",
      "<take_away>Gunmen from Laredo are not a physical entity but a fictional group, so their association with a narrator of \"Frontier\" is not grounded in physical principles.</take_away>\n",
      "<counterpoint>Physical principles do not apply to fictional narratives, so the question is not scientifically valid.</counterpoint>\n",
      "<critique>As a physicist, I should have recognized that this is a question about media and not physics.</critique>\n",
      "<likelihood>20%</likelihood>\n",
      "</step>\n",
      "\n",
      "<step>\n",
      "<expert>Computer Scientist</expert>\n",
      "<take_away>The question is not about algorithms or data structures, so it is not relevant to computer science.</take_away>\n",
      "<counterpoint>Computer science is not applicable here, as the question is about media content.</counterpoint>\n",
      "<critique>As a computer scientist, I should have recognized that this is a question about media and not computer science.</critique>\n",
      "<likelihood>20%</likelihood>\n",
      "</step>\n",
      "\n",
      "<step>\n",
      "<expert>Mathematician</expert>\n",
      "<take_away>The question is not about mathematical theorems or proofs, so it is not relevant to mathematics.</take_away>\n",
      "<counterpoint>Mathematical rigor does not apply to fictional narratives, so the question is not mathematically valid.</counterpoint>\n",
      "<critique>As a mathematician, I should have recognized that this is a question about media and not mathematics.</critique>\n",
      "<likelihood>20%</likelihood>\n",
      "</step>\n",
      "\n",
      "<step>\n",
      "<expert>Political Philosopher</expert>\n",
      "<take_away>The question is not about ethical implications or societal impact, so it is not relevant to political philosophy.</take_away>\n",
      "<counterpoint>Political philosophy is not applicable here, as the question is about media content.</counterpoint>\n",
      "<critique>As a political philosopher, I should have recognized that this is a question about media and not political philosophy.</critique>\n",
      "<likelihood>20%</likelihood>\n",
      "</step>\n",
      "\n",
      "<step>\n",
      "<expert>Lawyer</expert>\n",
      "<take_away>The question is not about legal frameworks or regulatory considerations, so it is not relevant to law.</take_away>\n",
      "<counterpoint>Legal frameworks do not apply to fictional narratives, so the question is not legally valid.</counterpoint>\n",
      "<critique>As a lawyer, I should have recognized that this is a question about media and not law.</critique>\n",
      "<likelihood>20%</likelihood>\n",
      "</step>\n",
      "\n",
      "<step>\n",
      "<expert>Economist</expert>\n",
      "<take_away>The question is not about cost-benefit analysis or economic viability, so it is not relevant to economics.</take_away>\n",
      "<counterpoint>Economic viability does not apply to fictional narratives, so the question is not economically valid.</counterpoint>\n",
      "<critique>As an economist, I should have recognized that this is a question about media and not economics.</critique>\n",
      "<likelihood>20%</likelihood>\n",
      "</step>\n",
      "\n",
      "<step>\n",
      "<expert>Medical Doctor</expert>\n",
      "<take_away>The question is not about health implications or medical accuracy, so it is not relevant to medicine.</take_away>\n",
      "<counterpoint>Medical accuracy does not apply to fictional narratives, so the question is not medically valid.</counterpoint>\n",
      "<critique>As a medical doctor, I should have recognized that this is a question about media and not medicine.</critique>\n",
      "<likelihood>20%</likelihood>\n",
      "</step>\n",
      "</steps>\n",
      "</thinking>\n",
      "\n",
      "<final_answer>\n",
      "The question \"Gunmen from Laredo starred which narrator of 'Frontier'?\" is not a scientifically valid question and does not fall under the expertise of any of the listed fields. It is a question about media content, specifically about a fictional narrative. The correct answer is that the question is not valid within the context of the provided fields of expertise. The likelihood of the take-away's correctness is 20%.\n",
      "</final_answer>\n"
     ]
    }
   ],
   "source": [
    "# %% Demo agent\n",
    "def demo():\n",
    "    question = \"Gunmen from Laredo starred which narrator of \\\"Frontier\\\"?\"\n",
    "    success, answer = run_agent(question)\n",
    "    if success:\n",
    "        print(f\"Question: {question}\\nAnswer: {answer}\")\n",
    "    else:\n",
    "        print(f\"Error: {answer}\")\n",
    "\n",
    "demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
